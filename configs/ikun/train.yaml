pretrained_model_path: "./checkpoints/stable-diffusion-2-1-unclip-small" 
output_dir: "./outputs/ikun"
controlnet_pretrained_model_path: [checkpoints/controlnet-2-1-unclip-small-openposefull, checkpoints/controlnet-2-1-unclip-small-depth]
use_temporal_conv: True

train_data:
  video_dir: "data/ikun"
  prompt: "A man is playing basketball"
  n_sample_frames: 8
  width: 768 
  height: 768
  sample_start_idx: 0
  sample_frame_rate: 1
  condition: [openposefull, depth]
  video_suffix: .jpg
  condition_suffix: .png
  noise_level: 10000
  image_embed_drop: 0.1
  mask_dir: man.mask

validation_data:
  prompts:
    - "A man is playing a basketball on the beach, anime style"
    - "A man is playing a basketball on the beach, anime style"

  ref_images: 
    - "data/ikun/masked_zhongli.png"
    - "data/ikun/masked_zhongli.png"


  video_length: 8 # 24
  width: 768
  height: 768
  num_inference_steps: 50
  guidance_scale: 12.5
  use_inv_latent: True
  num_inv_steps: 50 #50
  noise_level: 0
  interpolate_embed_weight: 1.0 ## 1.0 means all use image embedding
  use_masks: true
  start_step: 0 ## start to use mask
  end_step: 50 ## end to use mask
  mask_mode: all # mask_mode: emb / latent / all
  mask_latent_fuse_mode: all # inverse or all
  source_background: false # using source background and changing the protagonist
  source_protagonist: false # using source protagonist and changing the background
  controlnet_conditioning_scale: [.5, .5]


learning_rate: 3e-5
train_batch_size: 1
max_train_steps: 200
checkpointing_steps: 200
validation_steps: 200
trainable_modules:
  - "attn1.to_q"
  - "attn2.to_q"
  - "attn_temp"

seed: 33
mixed_precision: fp16
use_8bit_adam: False
gradient_checkpointing: True
enable_xformers_memory_efficient_attention: True
